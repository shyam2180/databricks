            - name: Cache
  uses: actions/cache@v3.3.2
  with:
    # A list of files, directories, and wildcard patterns to cache and restore
    path: 
    # An explicit key for restoring and saving the cache
    key: 
    # An ordered list of keys to use for restoring stale cache if no cache hit occurred for key. Note `cache-hit` returns false in this case.
    restore-keys: # optional
    # The chunk size used to split up large files during upload, in bytes
    upload-chunk-size: # optional
    # An optional boolean when enabled, allows windows runners to save or restore caches that can be restored or saved respectively on other platforms
    enableCrossOsArchive: # optional, default is false
    # Fail the workflow if cache entry is not found
    fail-on-cache-miss: # optional, default is false
    # Check if a cache entry exists for the given input(s) (key, restore-keys) without downloading the cache
    lookup-only: # optional, default is false
          

on:
  push:
    branches:
      - main
 
jobs:
  build:
    runs-on: ubuntu-latest
 
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
 
    - name: Install Databricks CLI
      run: |
        pip install databricks-cli

    - name: Configure Databricks CLI
      run: |
          echo -e "${{ secrets.DATABRICKS_HOST }}\n${{ secrets.DATABRICKS_TOKEN }}" | databricks configure --token
      env:
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    - name: Debug Token and Host
      run: |
        echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN }}"
        echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST }}"

    - name: Verify Databricks CLI Configuration
      run: cat $HOME/.databrickscfg
 
    - name: Upload Notebook to Databricks
      run: |     
        # Set variables for Databricks workspace and local notebook path
        DATABRICKS_PATH="/Users/shyamkumarr@jmangroup.com/cicd"
        LOCAL_NOTEBOOK_PATH="C:\\Users\\ShyamKumarR\\OneDrive - JMAN Group Ltd\\Desktop\\test_cicd_output\\ci_cd.py"
        
        # Import notebook to Databricks workspace
        databricks workspace import -l PYTHON  "$LOCAL_NOTEBOOK_PATH" "$DATABRICKS_PATH"

    - name: Run Databricks Notebook
      run: |
        databricks workspace create --name 'MyJob' --folder '/Workspace/Folder/Path' --format SOURCE --language SOURCE --overwrite
        databricks jobs run-now --job-id <job-id> # Use the actual job ID
